# model architecture config 
vocab_size: 50304
max_seq_len: 2048
d_model: 768
d_hidden: 3072
n_heads: 12
n_layers: 20
theta: 10000.0
eps: 1e-5

# MoE
moe_dim: 2048
n_routed_experts: 2
n_shared_experts: 2
n_activated_experts: 1
moe_every_n_layers: 4

# MLA
kv_latent_dim: 128
q_latent_dim: 128
qk_nope_head_dim: 64
qk_rope_head_dim: 64
v_head_dim: 128
